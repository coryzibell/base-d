//! fiche format: model-readable structured data
//!
//! fiche is the model-readable sibling to carrier98. While carrier98 is opaque
//! (maximum density, model shuttles without parsing), fiche uses Unicode delimiters
//! so models can parse structure with minimal tokens.
//!
//! # Delimiters
//!
//! | Symbol | Unicode | Purpose |
//! |--------|---------|---------|
//! | `‚óâ` | U+25C9 | Row start (fisheye) |
//! | `‚îÉ` | U+2503 | Field separator (heavy pipe) |
//! | `‚óà` | U+25C8 | Array element separator |
//! | `‚àÖ` | U+2205 | Null value |
//! | `·Éª` | U+10FB | Nested object separator |
//!
//! # Format
//!
//! ```text
//! @{root}‚îÉ{field}:{type}‚îÉ{field}:{type}...
//! ‚óâ{value}‚îÉ{value}‚îÉ{value}...
//! ‚óâ{value}‚îÉ{value}‚îÉ{value}...
//! ```
//!
//! # Example
//!
//! ```text
//! @users‚îÉid:int‚îÉname:str‚îÉactive:bool
//! ‚óâ1‚îÉalice‚îÉtrue
//! ‚óâ2‚îÉbob‚îÉfalse
//! ```

use super::types::{
    FLAG_HAS_NULLS, FLAG_HAS_ROOT_KEY, FieldDef, FieldType, IntermediateRepresentation,
    SchemaError, SchemaHeader, SchemaValue,
};

// Fiche delimiters
pub const ROW_START: char = '‚óâ'; // U+25C9 fisheye
pub const FIELD_SEP: char = '‚îÉ'; // U+2503 heavy pipe
pub const ARRAY_SEP: char = '‚óà'; // U+25C8 diamond in diamond
pub const NULL_VALUE: &str = "‚àÖ"; // U+2205 empty set
pub const SPACE_MARKER: char = '‚ñì'; // U+2593 Dark Shade
pub const NEST_SEP: char = '·Éª'; // U+10FB Georgian paragraph separator

// Type names in fiche schema (legacy)
pub const TYPE_INT: &str = "int";
pub const TYPE_STR: &str = "str";
pub const TYPE_FLOAT: &str = "float";
pub const TYPE_BOOL: &str = "bool";

// Superscript type markers (spec 1.7+)
pub const TYPE_INT_SUPER: char = '‚Å±'; // U+2071 superscript i
pub const TYPE_STR_SUPER: char = 'À¢'; // U+02E2 modifier letter small s
pub const TYPE_FLOAT_SUPER: char = '·∂†'; // U+1DA0 modifier letter small f
pub const TYPE_BOOL_SUPER: char = '·µá'; // U+1D47 modifier letter small b

// Token map prefix
pub const TOKEN_MAP_PREFIX: char = '@';

/// Field name tokenization alphabet (spec 1.5+)
/// Priority: Runic (89 chars) ‚Üí Hieroglyphs (1072) ‚Üí Cuneiform (1024)
/// Ancient scripts avoid ASCII/digit collision and regex interference
pub mod tokens {
    /// Runic alphabet (U+16A0 ‚Äì U+16F8, 89 characters, BMP)
    /// Primary tokenization alphabet - most compact, BMP-only
    pub const RUNIC: &[char] = &[
        '·ö†', '·ö°', '·ö¢', '·ö£', '·ö§', '·ö•', '·ö¶', '·öß', '·ö®', '·ö©', '·ö™', '·ö´', '·ö¨', '·ö≠', '·öÆ', '·öØ', '·ö∞', '·ö±',
        '·ö≤', '·ö≥', '·ö¥', '·öµ', '·ö∂', '·ö∑', '·ö∏', '·öπ', '·ö∫', '·öª', '·öº', '·öΩ', '·öæ', '·öø', '·õÄ', '·õÅ', '·õÇ', '·õÉ',
        '·õÑ', '·õÖ', '·õÜ', '·õá', '·õà', '·õâ', '·õä', '·õã', '·õå', '·õç', '·õé', '·õè', '·õê', '·õë', '·õí', '·õì', '·õî', '·õï',
        '·õñ', '·õó', '·õò', '·õô', '·õö', '·õõ', '·õú', '·õù', '·õû', '·õü', '·õ†', '·õ°', '·õ¢', '·õ£', '·õ§', '·õ•', '·õ¶', '·õß',
        '·õ®', '·õ©', '·õ™', '·õ´', '·õ¨', '·õ≠', '·õÆ', '·õØ', '·õ∞', '·õ±', '·õ≤', '·õ≥', '·õ¥', '·õµ', '·õ∂', '·õ∑', '·õ∏',
    ];

    /// Get a token character by index, spanning all alphabets
    /// Returns None if index exceeds available tokens
    pub fn get_token(index: usize) -> Option<char> {
        if index < RUNIC.len() {
            Some(RUNIC[index])
        } else {
            // Hieroglyphs and Cuneiform would be added here for overflow
            // For now, return None if we exceed runic capacity
            None
        }
    }

    /// Check if a character is a valid token
    pub fn is_token(c: char) -> bool {
        RUNIC.contains(&c)
    }

    /// Get the index of a token character
    #[allow(dead_code)]
    pub fn token_index(c: char) -> Option<usize> {
        RUNIC.iter().position(|&t| t == c)
    }
}

/// Value tokenization alphabet (spec 1.8+)
/// Egyptian Hieroglyphs for repeated value compression
pub mod value_tokens {
    /// Hieroglyphs (U+13000 ‚Äì U+1342F, 1072 characters, SMP)
    /// Used for value tokenization to avoid collision with field tokens
    pub const HIEROGLYPH_START: char = '\u{13000}'; // ìÄÄ
    pub const HIEROGLYPH_END: char = '\u{1342F}'; // 1072 chars available

    /// Get a hieroglyph token by index
    pub fn get_token(index: usize) -> Option<char> {
        let code_point = HIEROGLYPH_START as u32 + index as u32;
        if code_point <= HIEROGLYPH_END as u32 {
            char::from_u32(code_point)
        } else {
            None
        }
    }

    /// Check if a character is a hieroglyph token
    pub fn is_token(c: char) -> bool {
        (HIEROGLYPH_START..=HIEROGLYPH_END).contains(&c)
    }

    /// Get the index of a hieroglyph token
    #[allow(dead_code)]
    pub fn token_index(c: char) -> Option<usize> {
        if is_token(c) {
            Some((c as u32 - HIEROGLYPH_START as u32) as usize)
        } else {
            None
        }
    }
}

/// Serialize IR to fiche format (with tokenization)
pub fn serialize(ir: &IntermediateRepresentation, minify: bool) -> Result<String, SchemaError> {
    serialize_full_options(ir, minify, true, true)
}

/// Serialize IR to fiche format without tokenization (human-readable field names)
pub fn serialize_readable(
    ir: &IntermediateRepresentation,
    minify: bool,
) -> Result<String, SchemaError> {
    serialize_full_options(ir, minify, false, false)
}

/// Serialize IR with field tokenization only (no value dictionary)
pub fn serialize_light(
    ir: &IntermediateRepresentation,
    minify: bool,
) -> Result<String, SchemaError> {
    serialize_full_options(ir, minify, true, false)
}

#[allow(dead_code)]
pub fn serialize_minified(ir: &IntermediateRepresentation) -> Result<String, SchemaError> {
    serialize_full_options(ir, true, true, true)
}

/// Serialize with minify, tokenization, and value dictionary options
fn serialize_full_options(
    ir: &IntermediateRepresentation,
    minify: bool,
    tokenize: bool,
    tokenize_values: bool,
) -> Result<String, SchemaError> {
    // For backward compatibility, delegate to old function if not tokenizing
    if !tokenize {
        return serialize_with_options(ir, minify);
    }

    let mut output = String::new();
    let line_sep = if minify { SPACE_MARKER } else { '\n' };

    // Build token map for field names
    let mut token_map: Vec<(char, &str)> = Vec::new();
    for (idx, field) in ir.header.fields.iter().enumerate() {
        if let Some(token) = tokens::get_token(idx) {
            token_map.push((token, &field.name));
        } else {
            // Fall back to non-tokenized if we run out of tokens
            return serialize_with_options(ir, minify);
        }
    }

    // Build value dictionary if tokenize_values is enabled
    let value_dict = if tokenize_values {
        build_value_dictionary(ir)
    } else {
        std::collections::HashMap::new()
    };

    // Field token map header: @·ö†=field1,·ö°=field2,...
    output.push(TOKEN_MAP_PREFIX);
    for (idx, (token, name)) in token_map.iter().enumerate() {
        if idx > 0 {
            output.push(',');
        }
        output.push(*token);
        output.push('=');
        output.push_str(name);
    }
    output.push(line_sep);

    // Value dictionary header (if present): @ìÄÄ=value1,ìÄÅ=value2,...
    if !value_dict.is_empty() {
        output.push(TOKEN_MAP_PREFIX);
        // Sort by token for deterministic output
        let mut sorted_values: Vec<_> = value_dict.iter().collect();
        sorted_values.sort_by_key(|(_, token)| **token);

        for (idx, (value, token)) in sorted_values.iter().enumerate() {
            if idx > 0 {
                output.push(',');
            }
            output.push(**token);
            output.push('=');
            output.push_str(value);
        }
        output.push(line_sep);
    }

    // Schema line with tokens: @{root}‚îÉ·ö†{type}‚îÉ·ö°{type}...
    output.push('@');
    if let Some(ref root_key) = ir.header.root_key {
        output.push_str(root_key);
    }

    // Add metadata annotation if present
    if let Some(ref metadata) = ir.header.metadata {
        output.push('[');
        let mut sorted_keys: Vec<&String> = metadata.keys().collect();
        sorted_keys.sort(); // Deterministic order for roundtrip
        for (idx, key) in sorted_keys.iter().enumerate() {
            if idx > 0 {
                output.push(',');
            }
            output.push_str(key);
            output.push('=');
            // Replace spaces with SPACE_MARKER in metadata values
            let value = metadata[*key].replace(' ', &SPACE_MARKER.to_string());
            output.push_str(&value);
        }
        output.push(']');
    }

    for (idx, field) in ir.header.fields.iter().enumerate() {
        output.push(FIELD_SEP);
        output.push(token_map[idx].0); // Token instead of field name
        // Array markers (name‚ü¶‚üß) are structural metadata - no type suffix
        if !field.name.ends_with("‚ü¶‚üß") {
            output.push_str(&field_type_to_str(&field.field_type));
        }
    }
    output.push(line_sep);

    // Data rows: ‚óâ{value}‚îÉ{value}...
    let field_count = ir.header.fields.len();
    for row in 0..ir.header.row_count {
        output.push(ROW_START);

        for (field_idx, field) in ir.header.fields.iter().enumerate() {
            if field_idx > 0 {
                output.push(FIELD_SEP);
            }

            // Check null bitmap
            if ir.is_null(row, field_idx) {
                output.push_str(NULL_VALUE);
            } else {
                let value_idx = row * field_count + field_idx;
                let value = &ir.values[value_idx];
                let value_str = value_to_str(value, &field.field_type);

                // Replace with token if value is in dictionary
                if let Some(&token) = value_dict.get(&value_str) {
                    output.push(token);
                } else {
                    output.push_str(&value_str);
                }
            }
        }
        if row < ir.header.row_count - 1 {
            output.push(line_sep);
        }
    }

    Ok(output)
}

/// Build value dictionary from IR
/// Returns map of value string -> hieroglyph token for values appearing 2+ times
fn build_value_dictionary(
    ir: &IntermediateRepresentation,
) -> std::collections::HashMap<String, char> {
    use std::collections::HashMap;

    // Count occurrences of each string value
    let mut value_counts: HashMap<String, usize> = HashMap::new();
    let field_count = ir.header.fields.len();

    for row in 0..ir.header.row_count {
        for field_idx in 0..ir.header.fields.len() {
            // Skip nulls
            if ir.is_null(row, field_idx) {
                continue;
            }

            let value_idx = row * field_count + field_idx;
            let value = &ir.values[value_idx];

            // Only tokenize string values (exclude numbers, bools, nulls)
            if let SchemaValue::String(s) = value {
                let value_str = s.replace(' ', &SPACE_MARKER.to_string());
                *value_counts.entry(value_str).or_insert(0) += 1;
            }
        }
    }

    // Build dictionary for values with count >= 2
    let mut dict: HashMap<String, char> = HashMap::new();
    let mut sorted_values: Vec<_> = value_counts
        .iter()
        .filter(|(_, count)| **count >= 2)
        .collect();

    // Sort by frequency descending (most common first)
    sorted_values.sort_by(|a, b| b.1.cmp(a.1).then_with(|| a.0.cmp(b.0)));

    for (idx, (value, _)) in sorted_values.iter().enumerate() {
        if let Some(token) = value_tokens::get_token(idx) {
            dict.insert((*value).clone(), token);
        } else {
            // Ran out of hieroglyph tokens (unlikely with 1072 available)
            break;
        }
    }

    dict
}

fn serialize_with_options(
    ir: &IntermediateRepresentation,
    minify: bool,
) -> Result<String, SchemaError> {
    let mut output = String::new();
    let line_sep = if minify { SPACE_MARKER } else { '\n' };

    // Schema line: @{root}[meta1=val1,meta2=val2]‚îÉ{field}:{type}...
    output.push('@');
    if let Some(ref root_key) = ir.header.root_key {
        output.push_str(root_key);
    }

    // Add metadata annotation if present
    if let Some(ref metadata) = ir.header.metadata {
        output.push('[');
        let mut sorted_keys: Vec<&String> = metadata.keys().collect();
        sorted_keys.sort(); // Deterministic order for roundtrip
        for (idx, key) in sorted_keys.iter().enumerate() {
            if idx > 0 {
                output.push(',');
            }
            output.push_str(key);
            output.push('=');
            // Replace spaces with SPACE_MARKER in metadata values
            let value = metadata[*key].replace(' ', &SPACE_MARKER.to_string());
            output.push_str(&value);
        }
        output.push(']');
    }

    for field in &ir.header.fields {
        output.push(FIELD_SEP);
        output.push_str(&field.name);
        // Array markers (name‚ü¶‚üß) are structural metadata - no type suffix
        // Other fields get superscript type marker
        if !field.name.ends_with("‚ü¶‚üß") {
            output.push_str(&field_type_to_str(&field.field_type));
        }
    }
    output.push(line_sep);

    // Data rows: ‚óâ{value}‚îÉ{value}...
    let field_count = ir.header.fields.len();
    for row in 0..ir.header.row_count {
        output.push(ROW_START);

        for (field_idx, field) in ir.header.fields.iter().enumerate() {
            if field_idx > 0 {
                output.push(FIELD_SEP);
            }

            // Check null bitmap
            if ir.is_null(row, field_idx) {
                output.push_str(NULL_VALUE);
            } else {
                let value_idx = row * field_count + field_idx;
                let value = &ir.values[value_idx];
                output.push_str(&value_to_str(value, &field.field_type));
            }
        }
        if row < ir.header.row_count - 1 {
            output.push(line_sep);
        }
    }

    Ok(output)
}

/// Parse fiche format to IR
/// Supports both tokenized and non-tokenized formats
pub fn parse(input: &str) -> Result<IntermediateRepresentation, SchemaError> {
    let input = input.trim();
    if input.is_empty() {
        return Err(SchemaError::InvalidInput("Empty fiche input".to_string()));
    }

    // Split into schema line and data
    let row_marker = ROW_START.to_string();
    let first_row_pos = input.find(&row_marker);

    let (schema_part, data_part) = if let Some(pos) = first_row_pos {
        (&input[..pos], &input[pos..])
    } else {
        return Err(SchemaError::InvalidInput(
            "No data rows found (missing ‚óâ row marker)".to_string(),
        ));
    };

    // Parse token maps if present (tokenized format)
    // Format: @·ö†=field1,·ö°=field2,...\n@ìÄÄ=value1,ìÄÅ=value2,...\n@root‚îÉ...
    // Or minified: @·ö†=field1,...‚ñì@ìÄÄ=value1,...‚ñì@root‚îÉ...
    let mut token_map: std::collections::HashMap<char, String> = std::collections::HashMap::new();
    let mut value_dict: std::collections::HashMap<char, String> = std::collections::HashMap::new();
    let schema_part = schema_part.trim();

    // Parse dictionary lines
    let effective_schema = {
        let lines: Vec<&str> = schema_part
            .split(['\n', SPACE_MARKER])
            .filter(|s| !s.is_empty())
            .collect();

        let mut schema_line_idx = 0;

        // Parse up to 2 dictionary lines (field dict, value dict)
        for (idx, line) in lines.iter().enumerate() {
            if !line.starts_with('@') || line.len() <= 1 {
                schema_line_idx = idx;
                break;
            }

            let after_at = &line[1..];
            let first_char = after_at.chars().next();

            if let Some(fc) = first_char {
                // Field token dictionary (runic)
                if tokens::is_token(fc) && after_at.contains('=') {
                    let map_content = after_at;
                    for pair in map_content.split(',') {
                        let parts: Vec<&str> = pair.splitn(2, '=').collect();
                        if parts.len() == 2 {
                            let token = parts[0].chars().next();
                            let name = parts[1].to_string();
                            if let Some(t) = token {
                                token_map.insert(t, name);
                            }
                        }
                    }
                    schema_line_idx = idx + 1;
                }
                // Value token dictionary (hieroglyphs)
                else if value_tokens::is_token(fc) && after_at.contains('=') {
                    let map_content = after_at;
                    for pair in map_content.split(',') {
                        let parts: Vec<&str> = pair.splitn(2, '=').collect();
                        if parts.len() == 2 {
                            let token = parts[0].chars().next();
                            let value = parts[1].to_string();
                            if let Some(t) = token {
                                value_dict.insert(t, value);
                            }
                        }
                    }
                    schema_line_idx = idx + 1;
                }
                // Schema line (not a dictionary)
                else {
                    schema_line_idx = idx;
                    break;
                }
            } else {
                schema_line_idx = idx;
                break;
            }
        }

        if schema_line_idx < lines.len() {
            lines[schema_line_idx..].join(&SPACE_MARKER.to_string())
        } else {
            return Err(SchemaError::InvalidInput(
                "No schema line found after dictionaries".to_string(),
            ));
        }
    };

    // Parse schema line (strip minified line separator if present)
    let schema_line = effective_schema.trim().trim_end_matches(SPACE_MARKER);
    if !schema_line.starts_with('@') {
        return Err(SchemaError::InvalidInput(
            "Schema line must start with @".to_string(),
        ));
    }

    let schema_content = &schema_line[1..]; // Remove @

    // Check for metadata annotation: @root[key=val,...]‚îÉfield:type...
    let (root_and_metadata, field_defs) = if let Some(sep_pos) = schema_content.find(FIELD_SEP) {
        (&schema_content[..sep_pos], &schema_content[sep_pos..])
    } else {
        return Err(SchemaError::InvalidInput(
            "Schema line must contain at least one field definition".to_string(),
        ));
    };

    // Parse root key and metadata
    let (root_key, metadata) = if let Some(bracket_start) = root_and_metadata.find('[') {
        let root = &root_and_metadata[..bracket_start];
        let root_key = if root.is_empty() {
            None
        } else {
            Some(root.to_string())
        };

        // Find matching closing bracket (handle nested brackets in JSON values)
        let meta_content = &root_and_metadata[bracket_start + 1..];
        let mut depth = 0;
        let mut bracket_end = None;
        for (idx, ch) in meta_content.char_indices() {
            match ch {
                '[' => depth += 1,
                ']' => {
                    if depth == 0 {
                        bracket_end = Some(idx);
                        break;
                    }
                    depth -= 1;
                }
                _ => {}
            }
        }

        if let Some(end_pos) = bracket_end {
            let meta_str = &meta_content[..end_pos];
            let mut metadata = std::collections::HashMap::new();

            // Parse key=value pairs (handle JSON arrays with commas)
            let mut current_key = String::new();
            let mut current_value = String::new();
            let mut in_value = false;
            let mut json_depth = 0;

            for ch in meta_str.chars() {
                match ch {
                    '=' if !in_value && json_depth == 0 => {
                        in_value = true;
                    }
                    '[' if in_value => {
                        json_depth += 1;
                        current_value.push(ch);
                    }
                    ']' if in_value => {
                        json_depth -= 1;
                        current_value.push(ch);
                    }
                    ',' if in_value && json_depth == 0 => {
                        // End of key=value pair
                        let key = current_key.trim().to_string();
                        let value = current_value.trim().replace(SPACE_MARKER, " ");
                        if !key.is_empty() {
                            metadata.insert(key, value);
                        }
                        current_key.clear();
                        current_value.clear();
                        in_value = false;
                    }
                    _ => {
                        if in_value {
                            current_value.push(ch);
                        } else {
                            current_key.push(ch);
                        }
                    }
                }
            }

            // Insert final pair
            if !current_key.is_empty() {
                let key = current_key.trim().to_string();
                let value = current_value.trim().replace(SPACE_MARKER, " ");
                metadata.insert(key, value);
            }

            (
                root_key,
                if metadata.is_empty() {
                    None
                } else {
                    Some(metadata)
                },
            )
        } else {
            return Err(SchemaError::InvalidInput(
                "Unclosed metadata bracket in schema".to_string(),
            ));
        }
    } else {
        // No metadata, check for root key
        let root = root_and_metadata.trim();
        let root_key = if root.is_empty() || root.contains(':') {
            None
        } else {
            Some(root.to_string())
        };
        (root_key, None)
    };

    // Parse field definitions
    let schema_parts: Vec<&str> = field_defs.split(FIELD_SEP).collect();
    let mut fields = Vec::new();

    for part in &schema_parts {
        if part.is_empty() {
            continue;
        }
        let (name, field_type) = parse_field_def_with_tokens(part, &token_map)?;
        fields.push(FieldDef::new(name, field_type));
    }

    if fields.is_empty() {
        return Err(SchemaError::InvalidInput(
            "No field definitions in schema".to_string(),
        ));
    }

    // Parse data rows
    let mut values = Vec::new();
    let mut null_positions = Vec::new();
    let mut row_count = 0;

    // Split by row marker
    for row_str in data_part.split(ROW_START) {
        // Trim whitespace and minified line separator
        let row_str = row_str.trim().trim_end_matches(SPACE_MARKER);
        if row_str.is_empty() {
            continue;
        }

        // Handle multiline content - find next row or end
        let row_values: Vec<&str> = split_row(row_str, &fields);

        if row_values.len() != fields.len() {
            return Err(SchemaError::InvalidInput(format!(
                "Row {} has {} values, expected {} fields",
                row_count,
                row_values.len(),
                fields.len()
            )));
        }

        for (field_idx, (value_str, field)) in row_values.iter().zip(fields.iter()).enumerate() {
            let value_str = value_str.trim();

            if value_str == NULL_VALUE {
                // For array fields, ‚àÖ means empty array, not null
                if matches!(field.field_type, FieldType::Array(_)) {
                    values.push(SchemaValue::Array(vec![]));
                } else {
                    null_positions.push(row_count * fields.len() + field_idx);
                    values.push(SchemaValue::Null);
                }
            } else {
                // Check if value is a single hieroglyph token
                let resolved_value = if value_str.len() == 1 || value_str.chars().count() == 1 {
                    let first_char = value_str.chars().next().unwrap();
                    if let Some(expanded) = value_dict.get(&first_char) {
                        expanded.as_str()
                    } else {
                        value_str
                    }
                } else {
                    value_str
                };

                let value = parse_value(resolved_value, &field.field_type)?;
                values.push(value);
            }
        }

        row_count += 1;
    }

    // Build header
    let mut header = SchemaHeader::new(row_count, fields);
    if root_key.is_some() {
        header.root_key = root_key;
        header.set_flag(FLAG_HAS_ROOT_KEY);
    }
    header.metadata = metadata;

    // Build null bitmap if we have nulls
    if !null_positions.is_empty() {
        header.set_flag(FLAG_HAS_NULLS);
        let bitmap_size = (row_count * header.fields.len()).div_ceil(8);
        let mut bitmap = vec![0u8; bitmap_size];

        for pos in null_positions {
            let byte_idx = pos / 8;
            let bit_idx = pos % 8;
            bitmap[byte_idx] |= 1 << bit_idx;
        }
        header.null_bitmap = Some(bitmap);
    }

    IntermediateRepresentation::new(header, values)
}

/// Convert FieldType to fiche type string (superscript format)
fn field_type_to_str(ft: &FieldType) -> String {
    match ft {
        FieldType::U64 | FieldType::I64 => TYPE_INT_SUPER.to_string(),
        FieldType::F64 => TYPE_FLOAT_SUPER.to_string(),
        FieldType::String => TYPE_STR_SUPER.to_string(),
        FieldType::Bool => TYPE_BOOL_SUPER.to_string(),
        FieldType::Null => TYPE_STR_SUPER.to_string(), // Nulls rendered as str type
        FieldType::Array(inner) => {
            // Inline primitive arrays: emit type‚ü¶‚üß
            let inner_str = field_type_to_str(inner);
            format!("{}‚ü¶‚üß", inner_str)
        }
        FieldType::Any => TYPE_STR_SUPER.to_string(),
    }
}

/// Parse fiche type string to FieldType
/// Supports both legacy (int, str, float, bool) and superscript (‚Å±, À¢, ·∂†, ·µá) formats
fn parse_type_str(s: &str) -> Result<FieldType, SchemaError> {
    // Support both old [] and new ‚ü¶‚üß syntax for backward compatibility
    if let Some(inner) = s.strip_suffix("‚ü¶‚üß").or_else(|| s.strip_suffix("[]")) {
        let inner_type = parse_type_str(inner)?;
        return Ok(FieldType::Array(Box::new(inner_type)));
    }

    // Superscript type markers (spec 1.7+)
    let type_int_super = TYPE_INT_SUPER.to_string();
    let type_str_super = TYPE_STR_SUPER.to_string();
    let type_float_super = TYPE_FLOAT_SUPER.to_string();
    let type_bool_super = TYPE_BOOL_SUPER.to_string();

    match s {
        // Legacy format
        TYPE_INT => Ok(FieldType::I64),
        TYPE_STR => Ok(FieldType::String),
        TYPE_FLOAT => Ok(FieldType::F64),
        TYPE_BOOL => Ok(FieldType::Bool),
        // Superscript format
        _ if s == type_int_super => Ok(FieldType::I64),
        _ if s == type_str_super => Ok(FieldType::String),
        _ if s == type_float_super => Ok(FieldType::F64),
        _ if s == type_bool_super => Ok(FieldType::Bool),
        // Nested content marker
        "@" => Ok(FieldType::Array(Box::new(FieldType::String))),
        _ => Err(SchemaError::InvalidInput(format!(
            "Unknown type '{}'. Valid types: int/‚Å±, str/À¢, float/·∂†, bool/·µá, @",
            s
        ))),
    }
}

/// Parse field definition with token map support
/// Resolves token characters to field names using the provided map
fn parse_field_def_with_tokens(
    s: &str,
    token_map: &std::collections::HashMap<char, String>,
) -> Result<(String, FieldType), SchemaError> {
    // Get the (possibly tokenized) name and type
    let (name_or_token, field_type) = parse_field_def(s)?;

    // Check if the name is a single-char token that needs resolution
    let chars: Vec<char> = name_or_token.chars().collect();
    if chars.len() == 1
        && let Some(resolved_name) = token_map.get(&chars[0])
    {
        return Ok((resolved_name.clone(), field_type));
    }

    // Not a token, use as-is
    Ok((name_or_token, field_type))
}

/// Parse field definition
/// Supports both formats:
/// - Legacy: "name:str", "tags:str[]", "tags:str‚ü¶‚üß"
/// - Superscript: "nameÀ¢", "tagsÀ¢‚ü¶‚üß"
/// - Tokenized: "·ö†À¢" (single runic char followed by type)
fn parse_field_def(s: &str) -> Result<(String, FieldType), SchemaError> {
    // First, try legacy format with `:` separator
    if let Some(colon_pos) = s.find(':') {
        let name = s[..colon_pos].trim().to_string();
        let field_type = parse_type_str(s[colon_pos + 1..].trim())?;
        return Ok((name, field_type));
    }

    // Superscript format: field name ends with type marker
    // Check for array suffix first (À¢‚ü¶‚üß, ‚Å±‚ü¶‚üß, etc.)
    let (base, is_array) = if let Some(stripped) = s.strip_suffix("‚ü¶‚üß") {
        (stripped, true)
    } else if let Some(stripped) = s.strip_suffix("[]") {
        (stripped, true)
    } else {
        (s, false)
    };

    // Now find the type marker at the end
    let type_markers = [
        (TYPE_STR_SUPER, FieldType::String),
        (TYPE_INT_SUPER, FieldType::I64),
        (TYPE_FLOAT_SUPER, FieldType::F64),
        (TYPE_BOOL_SUPER, FieldType::Bool),
    ];

    for (marker, field_type) in &type_markers {
        let marker_str = marker.to_string();
        if base.ends_with(&marker_str) {
            let name = base[..base.len() - marker_str.len()].trim().to_string();
            if name.is_empty() {
                return Err(SchemaError::InvalidInput(format!(
                    "Empty field name in '{}'",
                    s
                )));
            }
            let final_type = if is_array {
                FieldType::Array(Box::new(field_type.clone()))
            } else {
                field_type.clone()
            };
            return Ok((name, final_type));
        }
    }

    Err(SchemaError::InvalidInput(format!(
        "Invalid field definition '{}'. Expected format: name:type or nameÀ¢/‚Å±/·∂†/·µá",
        s
    )))
}

/// Convert SchemaValue to fiche string
fn value_to_str(value: &SchemaValue, field_type: &FieldType) -> String {
    match value {
        SchemaValue::U64(n) => n.to_string(),
        SchemaValue::I64(n) => n.to_string(),
        SchemaValue::F64(n) => {
            // Preserve integer-like floats without decimal
            if n.fract() == 0.0 && n.abs() < 1e15 {
                format!("{:.1}", n)
            } else {
                n.to_string()
            }
        }
        SchemaValue::String(s) => s.replace(' ', &SPACE_MARKER.to_string()),
        SchemaValue::Bool(b) => b.to_string(),
        SchemaValue::Null => NULL_VALUE.to_string(),
        SchemaValue::Array(arr) => {
            if arr.is_empty() {
                return NULL_VALUE.to_string();
            }

            let inner_type = if let FieldType::Array(inner) = field_type {
                inner.as_ref()
            } else {
                &FieldType::String
            };

            // Inline primitive arrays with ‚óà separator
            let elements: Vec<String> = arr.iter().map(|v| value_to_str(v, inner_type)).collect();

            elements.join(&ARRAY_SEP.to_string())
        }
    }
}

/// Parse value string to SchemaValue
fn parse_value(s: &str, field_type: &FieldType) -> Result<SchemaValue, SchemaError> {
    // Check for null marker first (applies to all types)
    if s == NULL_VALUE {
        return Ok(SchemaValue::Null);
    }

    match field_type {
        FieldType::U64 => s
            .parse::<u64>()
            .map(SchemaValue::U64)
            .map_err(|_| SchemaError::InvalidInput(format!("Invalid integer: '{}'", s))),
        FieldType::I64 => s
            .parse::<i64>()
            .map(SchemaValue::I64)
            .map_err(|_| SchemaError::InvalidInput(format!("Invalid integer: '{}'", s))),
        FieldType::F64 => s
            .parse::<f64>()
            .map(SchemaValue::F64)
            .map_err(|_| SchemaError::InvalidInput(format!("Invalid float: '{}'", s))),
        FieldType::String => Ok(SchemaValue::String(s.replace(SPACE_MARKER, " "))),
        FieldType::Bool => match s {
            "true" => Ok(SchemaValue::Bool(true)),
            "false" => Ok(SchemaValue::Bool(false)),
            _ => Err(SchemaError::InvalidInput(format!(
                "Invalid boolean: '{}'. Expected 'true' or 'false'",
                s
            ))),
        },
        FieldType::Null => Ok(SchemaValue::Null),
        FieldType::Array(inner) => {
            if s.is_empty() {
                return Ok(SchemaValue::Array(vec![]));
            }
            // Split by ARRAY_SEP (‚óà) for inline primitive arrays
            let elements: Result<Vec<_>, _> = s
                .split(ARRAY_SEP)
                .map(|elem| parse_value(elem.trim(), inner))
                .collect();
            elements.map(SchemaValue::Array)
        }
        FieldType::Any => Ok(SchemaValue::String(s.to_string())),
    }
}

/// Split a row string by field separator, handling the known field count
fn split_row<'a>(row_str: &'a str, fields: &[FieldDef]) -> Vec<&'a str> {
    let sep = FIELD_SEP.to_string();
    let parts: Vec<&str> = row_str.splitn(fields.len(), &sep).collect();
    parts
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_simple_roundtrip() {
        // Superscript format (spec 1.7+), non-tokenized for clarity
        let fiche = "@users‚îÉid‚Å±‚îÉnameÀ¢‚îÉactive·µá
‚óâ1‚îÉalice‚îÉtrue
‚óâ2‚îÉbob‚îÉfalse";

        let ir = parse(fiche).unwrap();
        assert_eq!(ir.header.row_count, 2);
        assert_eq!(ir.header.fields.len(), 3);
        assert_eq!(ir.header.root_key, Some("users".to_string()));

        // Use readable (non-tokenized) for roundtrip
        let output = serialize_readable(&ir, false).unwrap();
        assert_eq!(output, fiche);
    }

    #[test]
    fn test_tokenized_roundtrip() {
        // Non-tokenized input
        let fiche = "@users‚îÉid‚Å±‚îÉnameÀ¢‚îÉactive·µá
‚óâ1‚îÉalice‚îÉtrue
‚óâ2‚îÉbob‚îÉfalse";

        let ir = parse(fiche).unwrap();

        // Tokenized output should have token map
        let tokenized = serialize(&ir, false).unwrap();
        assert!(tokenized.starts_with("@·ö†=id,·ö°=name,·ö¢=active\n"));
        assert!(tokenized.contains("‚îÉ·ö†‚Å±‚îÉ·ö°À¢‚îÉ·ö¢·µá"));

        // Parse tokenized output back
        let ir2 = parse(&tokenized).unwrap();
        assert_eq!(ir2.header.row_count, 2);
        assert_eq!(ir2.header.fields.len(), 3);

        // Field names should be restored
        assert_eq!(ir2.header.fields[0].name, "id");
        assert_eq!(ir2.header.fields[1].name, "name");
        assert_eq!(ir2.header.fields[2].name, "active");
    }

    #[test]
    fn test_legacy_type_format_parsing() {
        // Legacy format (pre 1.7) should still parse
        let fiche = "@users‚îÉid:int‚îÉname:str‚îÉactive:bool
‚óâ1‚îÉalice‚îÉtrue
‚óâ2‚îÉbob‚îÉfalse";

        let ir = parse(fiche).unwrap();
        assert_eq!(ir.header.row_count, 2);
        assert_eq!(ir.header.fields.len(), 3);

        // Output uses superscript format (readable mode)
        let output = serialize_readable(&ir, false).unwrap();
        assert!(output.contains("id‚Å±"));
        assert!(output.contains("nameÀ¢"));
        assert!(output.contains("active·µá"));
    }

    #[test]
    fn test_arrays_legacy_syntax() {
        // Test backward compatibility with old str[] syntax
        let fiche = "@users‚îÉid:int‚îÉtags:str[]
‚óâ1‚îÉadmin‚óàeditor
‚óâ2‚îÉviewer";

        let ir = parse(fiche).unwrap();
        assert_eq!(ir.header.row_count, 2);

        // Check first row's tags
        if let Some(SchemaValue::Array(tags)) = ir.get_value(0, 1) {
            assert_eq!(tags.len(), 2);
        } else {
            panic!("Expected array");
        }

        // Output uses superscript + ‚ü¶‚üß syntax (readable mode)
        let output = serialize_readable(&ir, false).unwrap();
        assert!(output.contains("tagsÀ¢‚ü¶‚üß"));
    }

    #[test]
    fn test_arrays_new_bracket_syntax() {
        // Test new superscript + ‚ü¶‚üß syntax
        let fiche = "@users‚îÉid‚Å±‚îÉtagsÀ¢‚ü¶‚üß
‚óâ1‚îÉadmin‚óàeditor
‚óâ2‚îÉviewer";

        let ir = parse(fiche).unwrap();
        assert_eq!(ir.header.row_count, 2);

        // Check first row's tags
        if let Some(SchemaValue::Array(tags)) = ir.get_value(0, 1) {
            assert_eq!(tags.len(), 2);
        } else {
            panic!("Expected array");
        }

        // Roundtrip with superscript format (readable mode)
        let output = serialize_readable(&ir, false).unwrap();
        assert_eq!(output, fiche);
    }

    #[test]
    fn test_nulls() {
        let fiche = "@records‚îÉid‚Å±‚îÉscore·∂†‚îÉnotesÀ¢
‚óâ1‚îÉ95.5‚îÉ‚àÖ
‚óâ2‚îÉ‚àÖ‚îÉpending";

        let ir = parse(fiche).unwrap();
        assert!(ir.is_null(0, 2)); // notes is null for row 0
        assert!(ir.is_null(1, 1)); // score is null for row 1

        let output = serialize_readable(&ir, false).unwrap();
        assert_eq!(output, fiche);
    }

    #[test]
    fn test_embedded_json() {
        let fiche = r#"@logs‚îÉlevelÀ¢‚îÉmsgÀ¢
‚óâerror‚îÉFailed‚ñìto‚ñìparse‚ñì{"key":‚ñì"value"}"#;

        let ir = parse(fiche).unwrap();

        if let Some(SchemaValue::String(msg)) = ir.get_value(0, 1) {
            assert_eq!(msg, r#"Failed to parse {"key": "value"}"#);
        } else {
            panic!("Expected string");
        }

        let output = serialize_readable(&ir, false).unwrap();
        assert_eq!(output, fiche);
    }

    #[test]
    fn test_no_root_key() {
        let fiche = "@‚îÉid‚Å±‚îÉnameÀ¢
‚óâ1‚îÉalice";

        let ir = parse(fiche).unwrap();
        assert_eq!(ir.header.root_key, None);
    }

    #[test]
    fn test_type_parsing() {
        // Legacy format
        assert!(matches!(parse_type_str("int"), Ok(FieldType::I64)));
        assert!(matches!(parse_type_str("str"), Ok(FieldType::String)));
        assert!(matches!(parse_type_str("float"), Ok(FieldType::F64)));
        assert!(matches!(parse_type_str("bool"), Ok(FieldType::Bool)));

        // Superscript format
        assert!(matches!(parse_type_str("‚Å±"), Ok(FieldType::I64)));
        assert!(matches!(parse_type_str("À¢"), Ok(FieldType::String)));
        assert!(matches!(parse_type_str("·∂†"), Ok(FieldType::F64)));
        assert!(matches!(parse_type_str("·µá"), Ok(FieldType::Bool)));

        // Test legacy [] syntax
        assert!(matches!(
            parse_type_str("str[]"),
            Ok(FieldType::Array(box_inner)) if *box_inner == FieldType::String
        ));
        // Test new ‚ü¶‚üß syntax with legacy type
        assert!(matches!(
            parse_type_str("str‚ü¶‚üß"),
            Ok(FieldType::Array(box_inner)) if *box_inner == FieldType::String
        ));
        // Test superscript + ‚ü¶‚üß syntax
        assert!(matches!(
            parse_type_str("À¢‚ü¶‚üß"),
            Ok(FieldType::Array(box_inner)) if *box_inner == FieldType::String
        ));
    }

    #[test]
    fn test_nested_arrays() {
        // Inline primitive arrays use ‚óà separator
        let fiche = "@people‚îÉnameÀ¢‚îÉheightÀ¢‚îÉfilmsÀ¢‚ü¶‚üß‚îÉvehiclesÀ¢‚ü¶‚üß
‚óâLuke‚îÉ172‚îÉfilm/1‚óàfilm/2‚îÉ‚àÖ
‚óâLeia‚îÉ150‚îÉfilm/1‚îÉvehicle/30";

        let ir = parse(fiche).unwrap();
        assert_eq!(ir.header.row_count, 2);
        assert_eq!(ir.header.fields.len(), 4);

        // Check Luke's name (scalar string)
        if let Some(SchemaValue::String(name)) = ir.get_value(0, 0) {
            assert_eq!(name, "Luke");
        } else {
            panic!("Expected string");
        }

        // Check Luke's height
        if let Some(SchemaValue::String(height)) = ir.get_value(0, 1) {
            assert_eq!(height, "172");
        } else {
            panic!("Expected string");
        }

        // Check Luke's films (array)
        if let Some(SchemaValue::Array(films)) = ir.get_value(0, 2) {
            assert_eq!(films.len(), 2);
            if let SchemaValue::String(film) = &films[0] {
                assert_eq!(film, "film/1");
            } else {
                panic!("Expected string");
            }
        } else {
            panic!("Expected array");
        }

        // Check Luke's empty vehicles
        if let Some(SchemaValue::Array(vehicles)) = ir.get_value(0, 3) {
            assert_eq!(vehicles.len(), 0);
        } else {
            panic!("Expected array");
        }

        // Check Leia's vehicles
        if let Some(SchemaValue::Array(vehicles)) = ir.get_value(1, 3) {
            assert_eq!(vehicles.len(), 1);
            if let SchemaValue::String(vehicle) = &vehicles[0] {
                assert_eq!(vehicle, "vehicle/30");
            } else {
                panic!("Expected string");
            }
        } else {
            panic!("Expected array");
        }

        let output = serialize_readable(&ir, false).unwrap();
        assert_eq!(output, fiche);
    }

    #[test]
    fn test_space_preservation() {
        let fiche = "@people‚îÉnameÀ¢‚îÉhomeÀ¢
‚óâLuke‚ñìSkywalker‚îÉTatooine‚ñìDesert‚ñìPlanet
‚óâLeia‚ñìOrgana‚îÉAlderaan";

        let ir = parse(fiche).unwrap();
        assert_eq!(ir.header.row_count, 2);

        // Check decoded values have spaces
        if let Some(SchemaValue::String(name)) = ir.get_value(0, 0) {
            assert_eq!(name, "Luke Skywalker");
        } else {
            panic!("Expected string");
        }

        if let Some(SchemaValue::String(home)) = ir.get_value(0, 1) {
            assert_eq!(home, "Tatooine Desert Planet");
        } else {
            panic!("Expected string");
        }

        // Check re-encoding produces minified spaces (readable mode)
        let output = serialize_readable(&ir, false).unwrap();
        assert!(output.contains("Luke‚ñìSkywalker"));
        assert!(output.contains("Tatooine‚ñìDesert‚ñìPlanet"));
        assert_eq!(output, fiche);
    }

    #[test]
    fn test_minified_output() {
        // Test that minified output uses ‚ñì for line breaks and roundtrips correctly
        let fiche_normal = "@users‚îÉid‚Å±‚îÉnameÀ¢
‚óâ1‚îÉalice
‚óâ2‚îÉbob";

        let ir = parse(fiche_normal).unwrap();

        // Serialize minified (tokenized) - check structure
        let minified = serialize_minified(&ir).unwrap();
        assert!(!minified.contains('\n'), "Minified should have no newlines");
        assert!(
            minified.contains('‚ñì'),
            "Minified should use ‚ñì as line separator"
        );

        // Parse minified back - should restore field names
        let ir2 = parse(&minified).unwrap();
        assert_eq!(ir2.header.row_count, 2);
        assert_eq!(ir2.header.fields[0].name, "id");
        assert_eq!(ir2.header.fields[1].name, "name");

        // Values should match
        if let Some(SchemaValue::String(name)) = ir2.get_value(0, 1) {
            assert_eq!(name, "alice");
        } else {
            panic!("Expected string");
        }
        if let Some(SchemaValue::String(name)) = ir2.get_value(1, 1) {
            assert_eq!(name, "bob");
        } else {
            panic!("Expected string");
        }
    }

    #[test]
    fn test_metadata_annotation() {
        let fiche = "@students[class=Year‚ñì1,school_name=Springfield‚ñìHigh]‚îÉidÀ¢
‚óâA1
‚óâB2";

        let ir = parse(fiche).unwrap();
        assert_eq!(ir.header.root_key, Some("students".to_string()));
        assert_eq!(ir.header.row_count, 2);

        // Check metadata
        assert!(ir.header.metadata.is_some());
        let metadata = ir.header.metadata.as_ref().unwrap();
        assert_eq!(
            metadata.get("school_name"),
            Some(&"Springfield High".to_string())
        );
        assert_eq!(metadata.get("class"), Some(&"Year 1".to_string()));

        // Check roundtrip (readable mode)
        let output = serialize_readable(&ir, false).unwrap();
        assert!(output.contains("[class=Year‚ñì1,school_name=Springfield‚ñìHigh]"));
        assert_eq!(output, fiche);
    }

    #[test]
    fn test_metadata_minified() {
        let fiche = "@students[class=Year‚ñì1,school_name=Springfield‚ñìHigh]‚îÉidÀ¢
‚óâA1
‚óâB2";

        let ir = parse(fiche).unwrap();
        assert_eq!(ir.header.row_count, 2);

        // Check metadata
        assert!(ir.header.metadata.is_some());
        let metadata = ir.header.metadata.as_ref().unwrap();
        assert_eq!(
            metadata.get("school_name"),
            Some(&"Springfield High".to_string())
        );
        assert_eq!(metadata.get("class"), Some(&"Year 1".to_string()));

        // Tokenized minified output roundtrips back to same structure
        let tokenized = serialize_minified(&ir).unwrap();
        let ir2 = parse(&tokenized).unwrap();
        assert_eq!(ir2.header.fields[0].name, "id");
        assert_eq!(ir2.header.row_count, 2);
    }

    #[test]
    fn test_value_dictionary() {
        // Service logs with repeated levels and service names
        let fiche = "@logs‚îÉlevelÀ¢‚îÉmessageÀ¢‚îÉserviceÀ¢
‚óâinfo‚îÉRequest‚ñìreceived‚îÉapi
‚óâdebug‚îÉParsing‚ñìpayload‚îÉapi
‚óâinfo‚îÉAuth‚ñìvalidated‚îÉapi
‚óâerror‚îÉConnection‚ñìtimeout‚îÉdb
‚óâinfo‚îÉResponse‚ñìsent‚îÉapi
‚óâerror‚îÉQuery‚ñìfailed‚îÉdb";

        let ir = parse(fiche).unwrap();
        assert_eq!(ir.header.row_count, 6);

        // Serialize with value dictionary (default for serialize)
        let tokenized = serialize(&ir, false).unwrap();

        // Should have field dictionary
        assert!(tokenized.starts_with("@·ö†=level,·ö°=message,·ö¢=service\n"));

        // Should have value dictionary (info, api, error, db appear 2+ times)
        assert!(tokenized.contains("@ìÄÄ="));

        // Parse back and verify roundtrip
        let ir2 = parse(&tokenized).unwrap();
        assert_eq!(ir2.header.row_count, 6);
        assert_eq!(ir2.header.fields.len(), 3);

        // Check field names restored
        assert_eq!(ir2.header.fields[0].name, "level");
        assert_eq!(ir2.header.fields[1].name, "message");
        assert_eq!(ir2.header.fields[2].name, "service");

        // Check values decoded correctly
        if let Some(SchemaValue::String(level)) = ir2.get_value(0, 0) {
            assert_eq!(level, "info");
        } else {
            panic!("Expected string value");
        }

        if let Some(SchemaValue::String(service)) = ir2.get_value(0, 2) {
            assert_eq!(service, "api");
        } else {
            panic!("Expected string value");
        }

        // Verify error level in row 3
        if let Some(SchemaValue::String(level)) = ir2.get_value(3, 0) {
            assert_eq!(level, "error");
        } else {
            panic!("Expected string value");
        }
    }

    #[test]
    fn test_value_dictionary_no_duplicates() {
        // All unique values - should not generate value dictionary
        let fiche = "@data‚îÉidÀ¢‚îÉnameÀ¢
‚óâ1‚îÉalice
‚óâ2‚îÉbob
‚óâ3‚îÉcarol";

        let ir = parse(fiche).unwrap();
        let tokenized = serialize(&ir, false).unwrap();

        // Should have field dictionary
        assert!(tokenized.starts_with("@·ö†=id,·ö°=name\n"));

        // Should NOT have value dictionary (no repeated values)
        let lines: Vec<&str> = tokenized.lines().collect();
        assert_eq!(lines.len(), 5); // field dict, schema, 3 data rows
        assert!(!tokenized.contains("ìÄÄ")); // No hieroglyphs
    }
}
